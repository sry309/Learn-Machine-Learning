基础知识
============================================================

batch
------------------------------------------------------------
梯度下降算法每次参数更新有两种方式。

第一种方式是遍历全部数据集计算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都计算一遍，开销大、速度慢、不支持在线学习，被称为批梯度下降(Batch gradient descent)。

另一种方法是每进入一个数据就计算一次损失函数，然后求梯度更新参数，称之为随机梯度下降(stochastic gradient descent)。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。

为了克服两种方法的缺点，一般采用的一种折中手段，小批的梯度下降（mini-batch gradient decent），这种方法把数据分为若干个批，按批来更新参数。这样一来，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。

通常指的batch_size，都是mini-batch。

epochs
------------------------------------------------------------
epochs一般被定义为向前和向后传播中所有批次的单次训练迭代。
